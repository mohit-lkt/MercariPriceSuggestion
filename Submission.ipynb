{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mohit/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mohit/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "merdata = pd.read_csv('merdata1.csv', converters={'tokens': eval})\n",
    "test = pd.read_csv('testdata1.csv', converters={'tokens': eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>train_id</th>\n",
       "      <th>name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>item_description</th>\n",
       "      <th>general_cat</th>\n",
       "      <th>subcat_1</th>\n",
       "      <th>subcat_2</th>\n",
       "      <th>desc_len</th>\n",
       "      <th>tokens</th>\n",
       "      <th>logPrice</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1436222</td>\n",
       "      <td>Tarte rainforest after dark colored clay</td>\n",
       "      <td>1</td>\n",
       "      <td>Beauty/Makeup/Makeup Palettes</td>\n",
       "      <td>Tarte</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1</td>\n",
       "      <td>A brand-new, unused, unopened, undamaged item....</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>Makeup</td>\n",
       "      <td>Makeup Palettes</td>\n",
       "      <td>83</td>\n",
       "      <td>[brand, new, unused, unopened, undamaged, item...</td>\n",
       "      <td>3.610918</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>402989</td>\n",
       "      <td>Mac mineralize skin finish</td>\n",
       "      <td>3</td>\n",
       "      <td>Beauty/Makeup/Face</td>\n",
       "      <td>MAC</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Color is gold deposit, about 85% of product left</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>Makeup</td>\n",
       "      <td>Face</td>\n",
       "      <td>9</td>\n",
       "      <td>[color, gold, deposit, product, left]</td>\n",
       "      <td>2.772589</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>638275</td>\n",
       "      <td>White Case iPhone 6/6s</td>\n",
       "      <td>1</td>\n",
       "      <td>Electronics/Cell Phones &amp; Accessories/Cases, C...</td>\n",
       "      <td>Not known</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>New Ultra thin Candy TPU Silicone Rubber Soft</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>Cell Phones &amp; Accessories</td>\n",
       "      <td>Cases, Covers &amp; Skins</td>\n",
       "      <td>8</td>\n",
       "      <td>[new, ultra, thin, candy, tpu, silicone, rubbe...</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1113629</td>\n",
       "      <td>Victoria's Secret push-up plunge</td>\n",
       "      <td>3</td>\n",
       "      <td>Women/Underwear/Bras</td>\n",
       "      <td>Victoria's Secret</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>VS sexy little thing multi way bras both size ...</td>\n",
       "      <td>Women</td>\n",
       "      <td>Underwear</td>\n",
       "      <td>Bras</td>\n",
       "      <td>36</td>\n",
       "      <td>[sexy, little, thing, multi, way, bras, size, ...</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.669</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.8126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>328823</td>\n",
       "      <td>Disney Princess Toddler Boots Size 10</td>\n",
       "      <td>2</td>\n",
       "      <td>Kids/Girls 2T-5T/Shoes</td>\n",
       "      <td>Disney</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "      <td>New with out box!</td>\n",
       "      <td>Kids</td>\n",
       "      <td>Girls 2T-5T</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>4</td>\n",
       "      <td>[new, box]</td>\n",
       "      <td>2.639057</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037769</th>\n",
       "      <td>1037769</td>\n",
       "      <td>1037769</td>\n",
       "      <td>674578</td>\n",
       "      <td>Nike Dri fit hooded top</td>\n",
       "      <td>2</td>\n",
       "      <td>Women/Athletic Apparel/Shirts &amp; Tops</td>\n",
       "      <td>Nike</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Like new. Sz L. Thumb holes</td>\n",
       "      <td>Women</td>\n",
       "      <td>Athletic Apparel</td>\n",
       "      <td>Shirts &amp; Tops</td>\n",
       "      <td>6</td>\n",
       "      <td>[like, new, thumb, holes]</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.3612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037770</th>\n",
       "      <td>1037770</td>\n",
       "      <td>1037770</td>\n",
       "      <td>276890</td>\n",
       "      <td>Talk Red Hunter Boots</td>\n",
       "      <td>3</td>\n",
       "      <td>Women/Shoes/Boots</td>\n",
       "      <td>Hunter Boots</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0</td>\n",
       "      <td>❤️❤️Sadly reselling because they are a little ...</td>\n",
       "      <td>Women</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>Boots</td>\n",
       "      <td>20</td>\n",
       "      <td>[❤️❤️sadly, reselling, little, small, boots, t...</td>\n",
       "      <td>4.204693</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.4215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037771</th>\n",
       "      <td>1037771</td>\n",
       "      <td>1037771</td>\n",
       "      <td>1069670</td>\n",
       "      <td>NWT FREE SHIP CC 2Beanie Hat Beige+Taup</td>\n",
       "      <td>1</td>\n",
       "      <td>Men/Men's Accessories/Hats</td>\n",
       "      <td>Not known</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NWT Unisex Ribbed cable knit beanie slouchy th...</td>\n",
       "      <td>Men</td>\n",
       "      <td>Men's Accessories</td>\n",
       "      <td>Hats</td>\n",
       "      <td>57</td>\n",
       "      <td>[nwt, unisex, ribbed, cable, knit, beanie, slo...</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.0516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037772</th>\n",
       "      <td>1037772</td>\n",
       "      <td>1037772</td>\n",
       "      <td>35382</td>\n",
       "      <td>Winston cup snapback</td>\n",
       "      <td>3</td>\n",
       "      <td>Men/Men's Accessories/Hats</td>\n",
       "      <td>Not known</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>In great condition</td>\n",
       "      <td>Men</td>\n",
       "      <td>Men's Accessories</td>\n",
       "      <td>Hats</td>\n",
       "      <td>3</td>\n",
       "      <td>[great, condition]</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.6249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037773</th>\n",
       "      <td>1037773</td>\n",
       "      <td>1037773</td>\n",
       "      <td>110009</td>\n",
       "      <td>Old Navy Rockstar Skinny Cords 10</td>\n",
       "      <td>3</td>\n",
       "      <td>Women/Jeans/Slim, Skinny</td>\n",
       "      <td>Old Navy</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Dark burgundy Size 10 Excellent like new condi...</td>\n",
       "      <td>Women</td>\n",
       "      <td>Jeans</td>\n",
       "      <td>Slim, Skinny</td>\n",
       "      <td>8</td>\n",
       "      <td>[dark, burgundy, size, excellent, like, new, c...</td>\n",
       "      <td>2.564949</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.7351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1037774 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0  Unnamed: 0.1  train_id  \\\n",
       "0                 0             0   1436222   \n",
       "1                 1             1    402989   \n",
       "2                 2             2    638275   \n",
       "3                 3             3   1113629   \n",
       "4                 4             4    328823   \n",
       "...             ...           ...       ...   \n",
       "1037769     1037769       1037769    674578   \n",
       "1037770     1037770       1037770    276890   \n",
       "1037771     1037771       1037771   1069670   \n",
       "1037772     1037772       1037772     35382   \n",
       "1037773     1037773       1037773    110009   \n",
       "\n",
       "                                             name  item_condition_id  \\\n",
       "0        Tarte rainforest after dark colored clay                  1   \n",
       "1                      Mac mineralize skin finish                  3   \n",
       "2                          White Case iPhone 6/6s                  1   \n",
       "3                Victoria's Secret push-up plunge                  3   \n",
       "4           Disney Princess Toddler Boots Size 10                  2   \n",
       "...                                           ...                ...   \n",
       "1037769                   Nike Dri fit hooded top                  2   \n",
       "1037770                     Talk Red Hunter Boots                  3   \n",
       "1037771   NWT FREE SHIP CC 2Beanie Hat Beige+Taup                  1   \n",
       "1037772                      Winston cup snapback                  3   \n",
       "1037773         Old Navy Rockstar Skinny Cords 10                  3   \n",
       "\n",
       "                                             category_name         brand_name  \\\n",
       "0                            Beauty/Makeup/Makeup Palettes              Tarte   \n",
       "1                                       Beauty/Makeup/Face                MAC   \n",
       "2        Electronics/Cell Phones & Accessories/Cases, C...          Not known   \n",
       "3                                     Women/Underwear/Bras  Victoria's Secret   \n",
       "4                                   Kids/Girls 2T-5T/Shoes             Disney   \n",
       "...                                                    ...                ...   \n",
       "1037769               Women/Athletic Apparel/Shirts & Tops               Nike   \n",
       "1037770                                  Women/Shoes/Boots       Hunter Boots   \n",
       "1037771                         Men/Men's Accessories/Hats          Not known   \n",
       "1037772                         Men/Men's Accessories/Hats          Not known   \n",
       "1037773                           Women/Jeans/Slim, Skinny           Old Navy   \n",
       "\n",
       "         price  shipping                                   item_description  \\\n",
       "0         36.0         1  A brand-new, unused, unopened, undamaged item....   \n",
       "1         15.0         1   Color is gold deposit, about 85% of product left   \n",
       "2          3.0         1      New Ultra thin Candy TPU Silicone Rubber Soft   \n",
       "3         18.0         1  VS sexy little thing multi way bras both size ...   \n",
       "4         13.0         1                                  New with out box!   \n",
       "...        ...       ...                                                ...   \n",
       "1037769   20.0         1                        Like new. Sz L. Thumb holes   \n",
       "1037770   66.0         0  ❤️❤️Sadly reselling because they are a little ...   \n",
       "1037771   20.0         1  NWT Unisex Ribbed cable knit beanie slouchy th...   \n",
       "1037772    8.0         0                                 In great condition   \n",
       "1037773   12.0         0  Dark burgundy Size 10 Excellent like new condi...   \n",
       "\n",
       "         general_cat                   subcat_1               subcat_2  \\\n",
       "0             Beauty                     Makeup        Makeup Palettes   \n",
       "1             Beauty                     Makeup                   Face   \n",
       "2        Electronics  Cell Phones & Accessories  Cases, Covers & Skins   \n",
       "3              Women                  Underwear                   Bras   \n",
       "4               Kids                Girls 2T-5T                  Shoes   \n",
       "...              ...                        ...                    ...   \n",
       "1037769        Women           Athletic Apparel          Shirts & Tops   \n",
       "1037770        Women                      Shoes                  Boots   \n",
       "1037771          Men          Men's Accessories                   Hats   \n",
       "1037772          Men          Men's Accessories                   Hats   \n",
       "1037773        Women                      Jeans           Slim, Skinny   \n",
       "\n",
       "         desc_len                                             tokens  \\\n",
       "0              83  [brand, new, unused, unopened, undamaged, item...   \n",
       "1               9              [color, gold, deposit, product, left]   \n",
       "2               8  [new, ultra, thin, candy, tpu, silicone, rubbe...   \n",
       "3              36  [sexy, little, thing, multi, way, bras, size, ...   \n",
       "4               4                                         [new, box]   \n",
       "...           ...                                                ...   \n",
       "1037769         6                          [like, new, thumb, holes]   \n",
       "1037770        20  [❤️❤️sadly, reselling, little, small, boots, t...   \n",
       "1037771        57  [nwt, unisex, ribbed, cable, knit, beanie, slo...   \n",
       "1037772         3                                 [great, condition]   \n",
       "1037773         8  [dark, burgundy, size, excellent, like, new, c...   \n",
       "\n",
       "         logPrice  negative  neutral  positive  compound  \n",
       "0        3.610918     0.036    0.928     0.036    0.0000  \n",
       "1        2.772589     0.000    1.000     0.000    0.0000  \n",
       "2        1.386294     0.000    1.000     0.000    0.0000  \n",
       "3        2.944439     0.000    0.669     0.331    0.8126  \n",
       "4        2.639057     0.000    1.000     0.000    0.0000  \n",
       "...           ...       ...      ...       ...       ...  \n",
       "1037769  3.044522     0.000    0.615     0.385    0.3612  \n",
       "1037770  4.204693     0.203    0.797     0.000   -0.4215  \n",
       "1037771  3.044522     0.099    0.807     0.094   -0.0516  \n",
       "1037772  2.197225     0.000    0.196     0.804    0.6249  \n",
       "1037773  2.564949     0.000    0.492     0.508    0.7351  \n",
       "\n",
       "[1037774 rows x 20 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deEmojify(inputString):\n",
    "    return inputString.encode('ascii', 'ignore').decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#operating on name column\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer1 = CountVectorizer(min_df = 10)\n",
    "vectorizer1.fit(merdata['name'].values)\n",
    "merdata_name = vectorizer1.transform(merdata['name'].values)\n",
    "test_name = vectorizer1.transform(test['name'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing item description length\n",
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "normalizer.fit(merdata['desc_len'].values.reshape(-1,1))\n",
    "\n",
    "x_train_words_des_norm = normalizer.transform(merdata['desc_len'].values.reshape(-1,1))\n",
    "x_test_words_des_norm = normalizer.transform(test['desc_len'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_words_des_norm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "x_dummies = csr_matrix(pd.get_dummies(merdata[['item_condition_id','shipping']],sparse=True).values)\n",
    "x_test_dummies = csr_matrix(pd.get_dummies(test[['item_condition_id','shipping']],sparse=True).values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = CountVectorizer(min_df = 10)\n",
    "vectorizer2.fit(merdata['brand_name'].values)\n",
    "merdata_brandname = vectorizer2.transform(merdata['brand_name'].values)\n",
    "test_brandname = vectorizer2.transform(test['brand_name'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General category Encoding\n",
    "vectorizer3 = CountVectorizer(min_df = 10)\n",
    "vectorizer3.fit(merdata['general_cat'].values)\n",
    "merdata_gencat = vectorizer3.transform(merdata['general_cat'].values)\n",
    "test_gencat = vectorizer3.transform(test['general_cat'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0           False\n",
       "train_id             False\n",
       "name                 False\n",
       "item_condition_id    False\n",
       "category_name        False\n",
       "brand_name           False\n",
       "price                False\n",
       "shipping             False\n",
       "item_description     False\n",
       "general_cat          False\n",
       "subcat_1             False\n",
       "subcat_2             False\n",
       "desc_len             False\n",
       "tokens               False\n",
       "logPrice             False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merdata.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sub category1 Encoding\n",
    "vectorizer4 = CountVectorizer(min_df = 10)\n",
    "vectorizer4.fit(merdata['subcat_1'].values)\n",
    "merdata_subcat1 = vectorizer4.transform(merdata['subcat_1'].values)\n",
    "test_subcat1 = vectorizer4.transform(test['subcat_1'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sub category2 Encoding\n",
    "vectorizer5 = CountVectorizer(min_df = 10)\n",
    "vectorizer5.fit(merdata['subcat_2'].values)\n",
    "merdata_subcat2 = vectorizer5.transform(merdata['subcat_2'].values)\n",
    "test_subcat2 = vectorizer5.transform(test['subcat_2'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1037774/1037774 [01:24<00:00, 12268.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'like green excellent used condition no flaws piling stains'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "preprocessed_train_des = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(merdata['item_description'].values):\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n",
    "    preprocessed_train_des.append(sent.lower().strip())\n",
    "\n",
    "# after preprocesing\n",
    "preprocessed_train_des[20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessed_train_des' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-75ac0a2dc8d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreprocessed_train_des\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocessed_train_des' is not defined"
     ]
    }
   ],
   "source": [
    "preprocessed_train_des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 444761/444761 [00:36<00:00, 12050.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'royal purple black cute lingerie native 34b fair condition pink vs garter medium nwt polka push vs 32d'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "preprocessed_test_des = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(test['item_description'].values):\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n",
    "    preprocessed_test_des.append(sent.lower().strip())\n",
    "\n",
    "# after preprocesing\n",
    "preprocessed_test_des[20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/mohit/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "100%|██████████| 1037774/1037774 [03:45<00:00, 4601.54it/s]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "train_sentiment = []; \n",
    "for sentence in tqdm(preprocessed_train_des):\n",
    "    for_sentiment = sentence\n",
    "    ss = sid.polarity_scores(for_sentiment)\n",
    "    train_sentiment.append(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative=[]\n",
    "neutral=[]\n",
    "positive=[]\n",
    "compounding=[]\n",
    "for i in train_sentiment:\n",
    "    \n",
    "    for polarity,score in i.items():\n",
    "        if(polarity=='neg'):\n",
    "            negative.append(score)\n",
    "        if(polarity=='neu'):\n",
    "            neutral.append(score)\n",
    "        if(polarity=='pos'):\n",
    "            positive.append(score)\n",
    "        if(polarity=='compound'):\n",
    "            compounding.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "merdata['negative']=negative\n",
    "merdata['neutral']=neutral\n",
    "merdata['positive']=positive\n",
    "merdata['compound']=compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/mohit/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "100%|██████████| 444761/444761 [01:36<00:00, 4596.68it/s]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "X_test_sentiment = []; \n",
    "for sentence in tqdm(preprocessed_test_des):\n",
    "    for_sentiment = sentence\n",
    "    ss = sid.polarity_scores(for_sentiment)\n",
    "    X_test_sentiment.append(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative=[]\n",
    "neutral=[]\n",
    "positive=[]\n",
    "compounding=[]\n",
    "for i in X_test_sentiment:\n",
    "    \n",
    "    for polarity,score in i.items():\n",
    "        if(polarity=='neg'):\n",
    "            negative.append(score)\n",
    "        if(polarity=='neu'):\n",
    "            neutral.append(score)\n",
    "        if(polarity=='pos'):\n",
    "            positive.append(score)\n",
    "        if(polarity=='compound'):\n",
    "            compounding.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['negative']=negative\n",
    "test['neutral']=neutral\n",
    "test['positive']=positive\n",
    "test['compound']=compounding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('testdata1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = merdata['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After normalizations\n",
      "(1037774, 1) (1037774,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "normalizer.fit(merdata['negative'].values.reshape(-1,1))\n",
    "\n",
    "#X_train_price_norm = normalizer.transform(X_train['price'].values.reshape(-1,1))\n",
    "X_train_neg_norm = normalizer.transform(merdata['negative'].values.reshape(-1,1))\n",
    "\n",
    "X_test_neg_norm = normalizer.transform(test['negative'].values.reshape(-1,1))\n",
    "\n",
    "print(\"After normalizations\")\n",
    "print(X_train_neg_norm.shape, y_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After normalizations\n",
      "(1037774, 1) (1037774,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "normalizer.fit(merdata['neutral'].values.reshape(-1,1))\n",
    "\n",
    "#X_train_price_norm = normalizer.transform(X_train['price'].values.reshape(-1,1))\n",
    "X_train_neu_norm = normalizer.transform(merdata['neutral'].values.reshape(-1,1))\n",
    "\n",
    "X_test_neu_norm = normalizer.transform(test['neutral'].values.reshape(-1,1))\n",
    "\n",
    "print(\"After normalizations\")\n",
    "print(X_train_neu_norm.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After normalizations\n",
      "(1037774, 1) (1037774,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "normalizer.fit(merdata['positive'].values.reshape(-1,1))\n",
    "\n",
    "#X_train_price_norm = normalizer.transform(X_train['price'].values.reshape(-1,1))\n",
    "X_train_pos_norm = normalizer.transform(merdata['positive'].values.reshape(-1,1))\n",
    "\n",
    "X_test_pos_norm = normalizer.transform(test['positive'].values.reshape(-1,1))\n",
    "\n",
    "print(\"After normalizations\")\n",
    "print(X_train_pos_norm.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After normalizations\n",
      "(1037774, 1) (1037774,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "normalizer.fit(merdata['compound'].values.reshape(-1,1))\n",
    "\n",
    "#X_train_price_norm = normalizer.transform(X_train['price'].values.reshape(-1,1))\n",
    "X_train_com_norm = normalizer.transform(merdata['compound'].values.reshape(-1,1))\n",
    "\n",
    "X_test_com_norm = normalizer.transform(test['compound'].values.reshape(-1,1))\n",
    "\n",
    "print(\"After normalizations\")\n",
    "print(X_train_com_norm.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=10,max_features=5000)\n",
    "vectorizer.fit(preprocessed_train_des)\n",
    "X_train_itemdes = vectorizer.transform(preprocessed_train_des)\n",
    "X_test_itemdes = vectorizer.transform(preprocessed_test_des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train matrix after one hot encodig  (1037774, 5000)\n",
      "Shape of train matrix after one hot encodig  (444761, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of train matrix after one hot encodig \",X_train_itemdes.shape)\n",
    "print(\"Shape of train matrix after one hot encodig \",X_test_itemdes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final data matrix\n",
      "(1037774, 22769) (1037774,)\n",
      "(444761, 22769)\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "X_train = hstack((merdata_name , merdata_brandname,merdata_gencat,merdata_subcat1,merdata_subcat2,X_train_itemdes,x_dummies,x_train_words_des_norm,X_train_neg_norm,X_train_neu_norm,X_train_pos_norm,X_train_com_norm)).tocsr()\n",
    "\n",
    "X_test = hstack((test_name , test_brandname,test_gencat,test_subcat1,test_subcat2,X_test_itemdes,x_test_dummies,x_test_words_des_norm,X_test_neg_norm,X_test_neu_norm,X_test_pos_norm,X_test_com_norm)).tocsr()\n",
    "y_train = merdata.price\n",
    "print(\"Final data matrix\")\n",
    "print(X_train.shape,y_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=0.02, fit_intercept=False, max_iter=200, solver='lsqr', tol=0.01)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_reg = Ridge(solver='lsqr', fit_intercept=False, alpha=0.02,max_iter=200, normalize=False, tol=0.01)\n",
    "ridge_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31.94287517, 52.77385551, 20.97622028, ..., 40.31837726,\n",
       "       43.80470818, 22.79596895])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = ridge_reg.predict(X_test)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([41.43861857, 52.27681976, 18.87527044, ..., 14.59358146,\n",
       "       37.06209213, 19.97182262])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = ridge_reg.predict(X_test)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "#from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\"alpha\":[0.01,0.1,0,1,10,100]}\n",
    "ridgeReg = Ridge(solver = \"lsqr\", fit_intercept=False)\n",
    "lr_reg = GridSearchCV(ridgeReg,param_grid =parameters,n_jobs=-1)\n",
    "lr_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohit/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_stochastic_gradient.py:1208: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\"Maximum number of iteration reached before \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SGDRegressor(fit_intercept=False, max_iter=200),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'alpha': [1e-08, 1e-05, 0.0001, 0.001, 0.01, 0.1, 0, 1,\n",
       "                                   10, 100, 1000, 10000, 100000],\n",
       "                         'l1_ratio': [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\"alpha\": [0.001,0.01,0.1,0,1,10] , \"l1_ratio\":[0.3,0,4,0.5,0.6,0.7]}\n",
    "\n",
    "model = SGDRegressor(loss = \"squared_loss\", learning_rate = \"invscaling\",max_iter = 200, penalty = \"l2\",fit_intercept = False)\n",
    "\n",
    "lr_reg = GridSearchCV(model,param_grid = parameters , n_jobs = -1)\n",
    "lr_reg.fit(X_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {'n_estimators': [10,50,100,150,160,200,300,350,400,500],\n",
    "               'min_samples_split': [2,3,5,6,7,8],\n",
    "               \n",
    "              \n",
    "              \"max_depth\":[None,10,20,40,60,80,100,120]\n",
    "             \n",
    "              }\n",
    "regr1 = RandomForestRegressor()\n",
    "n_iter_search = 100\n",
    "regr1 = RandomizedSearchCV(regr1, param_distributions=param_dist,\n",
    "                                     n_jobs=-1, n_iter=n_iter_search, cv=3)\n",
    "\n",
    "regr1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1470 candidates, totalling 4410 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "gridParams = {\n",
    "    'learning_rate': [ 0.1,0.2,0.3,0.4,0.5],\n",
    "    'n_estimators': [100,150, 200,250,300,400,500],\n",
    "    'num_leaves': [20,30,63,80,100,120],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    'max_depth' : [2,3,4,5,6,7,8]\n",
    "}\n",
    "lgbm_params ={'subsample': 0.9, 'colsample_bytree': 0.8, 'min_child_samples': 50, 'objective': 'regression'\n",
    "        }\n",
    "model = LGBMRegressor(**lgbm_params)\n",
    "# Create the grid\n",
    "grid = GridSearchCV(model, gridParams, verbose=1, cv=3, n_jobs=-1)\n",
    "# Run the grid\n",
    "grid.fit(X_train, y_train,\n",
    "         \n",
    "         early_stopping_rounds=100,\n",
    "         verbose=True)\n",
    "\n",
    "print('Best parameters found by grid search are:', grid.best_params_)\n",
    "print('Best score found by grid search is:', grid.best_score_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "For early stopping, at least one dataset and eval metric is required for evaluation",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-4731db2c96b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m       }\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLGBMRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlgbm_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m model.fit(X_train, y_train,\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    727\u001b[0m             verbose=True, feature_name='auto', categorical_feature='auto', callbacks=None):\n\u001b[1;32m    728\u001b[0m         \u001b[0;34m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m         super(LGBMRegressor, self).fit(X, y, sample_weight=sample_weight,\n\u001b[0m\u001b[1;32m    730\u001b[0m                                        \u001b[0minit_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                                        \u001b[0meval_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/lightgbm/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[1;32m    587\u001b[0m                 \u001b[0mvalid_sets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         self._Booster = train(params, train_set,\n\u001b[0m\u001b[1;32m    590\u001b[0m                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_sets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_sets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks_after_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m                 cb(callback.CallbackEnv(model=booster,\n\u001b[0m\u001b[1;32m    260\u001b[0m                                         \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                                         \u001b[0miteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/lightgbm/callback.py\u001b[0m in \u001b[0;36m_callback\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcmp_op\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0menabled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/lightgbm/callback.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_result_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             raise ValueError('For early stopping, '\n\u001b[0m\u001b[1;32m    196\u001b[0m                              'at least one dataset and eval metric is required for evaluation')\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: For early stopping, at least one dataset and eval metric is required for evaluation"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "lgbm_params ={'subsample': 0.9, 'colsample_bytree': 0.8, 'min_child_samples': 50, 'objective': 'regression','boosting_type': 'gbdt','learning_rate': 0.5,'max_depth': 8,'n_estimators': 500,'num_leaves': 80,\n",
    "      }\n",
    "model = LGBMRegressor(**lgbm_params)\n",
    "model.fit(X_train, y_train, early_stopping_rounds=100,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([42.25085769, 53.78152003, 18.84366425, ..., 23.28040138,\n",
       "       36.99231316, 14.80814919])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['price'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1048243</td>\n",
       "      <td>31.942875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86986</td>\n",
       "      <td>52.773856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>698316</td>\n",
       "      <td>20.976220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>268868</td>\n",
       "      <td>32.691255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1360398</td>\n",
       "      <td>40.439743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444756</th>\n",
       "      <td>316678</td>\n",
       "      <td>21.183912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444757</th>\n",
       "      <td>795624</td>\n",
       "      <td>21.494045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444758</th>\n",
       "      <td>765638</td>\n",
       "      <td>40.318377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444759</th>\n",
       "      <td>766765</td>\n",
       "      <td>43.804708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444760</th>\n",
       "      <td>1383311</td>\n",
       "      <td>22.795969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>444761 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id      price\n",
       "0       1048243  31.942875\n",
       "1         86986  52.773856\n",
       "2        698316  20.976220\n",
       "3        268868  32.691255\n",
       "4       1360398  40.439743\n",
       "...         ...        ...\n",
       "444756   316678  21.183912\n",
       "444757   795624  21.494045\n",
       "444758   765638  40.318377\n",
       "444759   766765  43.804708\n",
       "444760  1383311  22.795969\n",
       "\n",
       "[444761 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = test[['id','price']]\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z.to_csv('ninth_submission.csv',index = False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
